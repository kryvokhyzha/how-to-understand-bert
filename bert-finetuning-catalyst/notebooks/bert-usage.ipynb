{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT classifier fine-tuning with PyTorch, HuggingFace, and Catalyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading data and basic EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task is to classify articles into Sustainable Development Goals**\n",
    "<img src=\"../img/all_sdgs.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/sdg_classification/train_set_sdg_1_7_8_12_13_toy.csv')\n",
    "valid_df = pd.read_csv('../data/sdg_classification/val_set_sdg_1_7_8_12_13_toy.csv')\n",
    "test_df = pd.read_csv('../data/sdg_classification/eval_set_sdg_1_7_8_12_13_curated_journals_toy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 6 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   eid                      150 non-null    int64 \n",
      " 1   sdg_id                   150 non-null    int64 \n",
      " 2   title                    150 non-null    object\n",
      " 3   keywords                 150 non-null    object\n",
      " 4   abstract                 150 non-null    object\n",
      " 5   title_keywords_abstract  150 non-null    object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 7.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>sdg_id</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_keywords_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84895022699</td>\n",
       "      <td>13</td>\n",
       "      <td>GIS-based risk assessment for the Nile Delta c...</td>\n",
       "      <td>GIS Inundation Sea level rise</td>\n",
       "      <td>Sea level changes are caused by several natura...</td>\n",
       "      <td>[TITLE] gis-based risk assessment for the nile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84978997581</td>\n",
       "      <td>1</td>\n",
       "      <td>Ritual well-being: toward a social signaling m...</td>\n",
       "      <td>Costly signaling religion and mental health re...</td>\n",
       "      <td>Religion is positively correlated with subject...</td>\n",
       "      <td>[TITLE] ritual well-being: toward a social sig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           eid  sdg_id                                              title  \\\n",
       "0  84895022699      13  GIS-based risk assessment for the Nile Delta c...   \n",
       "1  84978997581       1  Ritual well-being: toward a social signaling m...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0                      GIS Inundation Sea level rise   \n",
       "1  Costly signaling religion and mental health re...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Sea level changes are caused by several natura...   \n",
       "1  Religion is positively correlated with subject...   \n",
       "\n",
       "                             title_keywords_abstract  \n",
       "0  [TITLE] gis-based risk assessment for the nile...  \n",
       "1  [TITLE] ritual well-being: toward a social sig...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[TITLE] gis-based risk assessment for the nile delta coastal zone under different sea level rise scenarios case study: kafr el sheikh governorate, egypt [KEYWORDS] gis inundation sea level rise [ABSTRACT] sea level changes are caused by several natural phenomena, including mainly ocean thermal expansion, glacial melt from greenland and antarctica. it was estimated, in this respect, that global average sea level rose, during the 20th century, by at least 10 cm. this trend is expected to continue and most likely accelerated during the 21st century due to human-induced global warming. global average sea level is expected to rise, by the year 2100, due to global warming between 0.18 and 0.59 cm. such a rise in sea-level will significantly impact coastal areas due to the high concentration of natural and socioeconomic activities and assets located along the coast. the northern coastal zone of the nile delta is generally low land, and is consequently vulnerable to direct and indirect impacts of sea level rise (slr) due to climate changes, particularly inundation. despite the uncertainty associated with developed scenarios for climate change and expected slr, there is a need, according to precautionary approach, to assess and analyze the impacts of slr. such an assessment, on one hand, can assist in formulating effective adaptation options to specific, sometimes localized, impacts of slr. on the other hand, such an analysis can contribute significantly to the development of integrated approach to deal with the impacts of slr. the objective of this paper is to assess and spatially analyze the risks of expected sea level rise (slr), in particular inundation, and its implications up to the year 2100 in kafr el sheikh governorate, egypt, using gis techniques. for that purpose, a gis was developed for the study area and then utilized to identify the spatial extent of those areas that would be vulnerable to inundation by slr. moreover, various land uses/land covers susceptible to such inundation were identified. results indicate that more than 22.59 % and 24.50 % of the total area of kafr el sheikh governorate would be vulnerable to inundation under b1 and a1fi (ipcc most optimistic and pessimistic scenarios), respectively. no significant difference was noticed between the two scenarios in terms of spatial extent of slr impacts. it was also found that a significant proportion of these areas were found to be currently either undeveloped or wetlands. moreover, it was found that about 90.13 % of the vulnerable areas are actually less exposed to the risks of slr due to the existence of a number of man-made features, not intended as protection measures, e.g. international coastal highway, that can be used to limit the areas vulnerable to inundations by slr. Â© springer science+business media dordrecht 2013.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[0, 'title_keywords_abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13    33\n",
       "12    31\n",
       "8     31\n",
       "1     29\n",
       "7     26\n",
       "Name: sdg_id, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sdg_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13    12\n",
       "1     12\n",
       "8     11\n",
       "12    10\n",
       "7      5\n",
       "Name: sdg_id, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df['sdg_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     88\n",
       "8     46\n",
       "13     9\n",
       "12     7\n",
       "Name: sdg_id, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['sdg_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    150.000000\n",
       "mean     203.773333\n",
       "std       78.735147\n",
       "min       58.000000\n",
       "25%      148.000000\n",
       "50%      191.000000\n",
       "75%      234.750000\n",
       "max      564.000000\n",
       "Name: title_keywords_abstract, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have limitation of 512 tokens (for basic implementation)\n",
    "\n",
    "train_df['title_keywords_abstract'].apply(\n",
    "    lambda s: len(s.split())).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Mapping, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wrapper around Torch Dataset to perform text classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: List[str] = None,\n",
    "        label_dict: Mapping[str, int] = None,\n",
    "        max_seq_length: int = 512,\n",
    "        model_name: str = \"distilbert-base-uncased\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (List[str]): a list with texts to classify or to train the\n",
    "                classifier on\n",
    "            labels List[str]: a list with classification labels (optional)\n",
    "            label_dict (dict): a dictionary mapping class names to class ids,\n",
    "                to be passed to the validation data (optional)\n",
    "            max_seq_length (int): maximal sequence length in tokens,\n",
    "                texts will be stripped to this length\n",
    "            model_name (str): transformer model name, needed to perform\n",
    "                appropriate tokenization\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.label_dict = label_dict\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        if self.label_dict is None and labels is not None:\n",
    "            # {'class1': 0, 'class2': 1, 'class3': 2, ...}\n",
    "            # using this instead of `sklearn.preprocessing.LabelEncoder`\n",
    "            # no easily handle unknown target values\n",
    "            self.label_dict = dict(zip(sorted(set(labels)), range(len(set(labels)))))\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # suppresses tokenizer warnings\n",
    "        logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.FATAL)\n",
    "\n",
    "        # special tokens for transformers\n",
    "        # in the simplest case a [CLS] token is added in the beginning\n",
    "        # and [SEP] token is added in the end of a piece of text\n",
    "        # [CLS] <indexes text tokens> [SEP] .. <[PAD]>\n",
    "        self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n",
    "        self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n",
    "        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n",
    "        \"\"\"Gets element of the dataset\n",
    "\n",
    "        Args:\n",
    "            index (int): index of the element in the dataset\n",
    "        Returns:\n",
    "            Single element by index\n",
    "        \"\"\"\n",
    "\n",
    "        # encoding the text\n",
    "        x = self.texts[index]\n",
    "\n",
    "        # a dictionary with `input_ids` and `attention_mask` as keys\n",
    "        output_dict = self.tokenizer.encode_plus(\n",
    "            x,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # for Catalyst, there needs to be a key called features\n",
    "        output_dict[\"features\"] = output_dict[\"input_ids\"].squeeze(0)\n",
    "        del output_dict[\"input_ids\"]\n",
    "\n",
    "        # encoding target\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[index]\n",
    "            y_encoded = torch.Tensor([self.label_dict.get(y, -1)]).long().squeeze(0)\n",
    "            output_dict[\"targets\"] = y_encoded\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'yidude! I enjoy playing football under rain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = tokenizer.encode_plus(\n",
    "            input_text,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=16,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 12316,  8566,  3207,   999,  1045,  5959,  2652,  2374,  2104,\n",
       "          4542,   102,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = tokenizer.get_vocab()\n",
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_voc = {v: k for (k, v) in voc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] yi ##du ##de ! i enjoy playing football under rain [SEP] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wordpiece tokenization bert\n",
    "\n",
    "' '.join([inv_voc[i] for i in output_dict['input_ids'].tolist()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict['attention_mask'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextClassificationDataset(\n",
    "        texts=train_df['title_keywords_abstract'].values.tolist(),\n",
    "        labels=train_df['sdg_id'].values,\n",
    "        max_seq_length=16,\n",
    "        model_name=MODEL_NAME,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = TextClassificationDataset(\n",
    "        texts=valid_df['title_keywords_abstract'].values.tolist(),\n",
    "        labels=valid_df['sdg_id'].values,\n",
    "        max_seq_length=16,\n",
    "        model_name=MODEL_NAME,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'features': tensor([  101,  1031,  2516,  1033, 10808,  3169, 19940,  5549,  1998, 20600,\n",
       "         2241,  2006,  3612,  2373,  1998,   102]), 'targets': tensor(1)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, 7: 1, 8: 2, 12: 3, 13: 4}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loaders = {\n",
    "        \"train\": DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "        ),\n",
    "        \"valid\": DataLoader(\n",
    "            dataset=valid_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fe24280c100>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_loaders['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_val_loaders['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_val_loaders['train']))['features'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified version of the same class by HuggingFace.\n",
    "    See transformers/modeling_distilbert.py in the transformers repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_name: str,\n",
    "        num_classes: int = None,\n",
    "        dropout: float = 0.3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pretrained_model_name (str): HuggingFace model name.\n",
    "                See transformers/modeling_auto.py\n",
    "            num_classes (int): the number of class labels\n",
    "                in the classification task\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            pretrained_model_name, num_labels=num_classes\n",
    "        )\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(pretrained_model_name, config=config)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, features, attention_mask=None, head_mask=None):\n",
    "        \"\"\"Compute class probabilities for the input sequence.\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): ids of each token,\n",
    "                size ([bs, seq_length]\n",
    "            attention_mask (torch.Tensor): binary tensor, used to select\n",
    "                tokens which are used to compute attention scores\n",
    "                in the self-attention heads, size [bs, seq_length]\n",
    "            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n",
    "                we keep the head, size: [num_heads]\n",
    "                or [num_hidden_layers x num_heads]\n",
    "        Returns:\n",
    "            PyTorch Tensor with predicted class scores\n",
    "        \"\"\"\n",
    "        assert attention_mask is not None, \"attention mask is none\"\n",
    "\n",
    "        # taking BERTModel output\n",
    "        # see https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel\n",
    "        bert_output = self.model(\n",
    "            input_ids=features, attention_mask=attention_mask, head_mask=head_mask\n",
    "        )\n",
    "        # we only need the hidden state here and don't need\n",
    "        # transformer output, so index 0\n",
    "        seq_output = bert_output[0]  # (bs, seq_len, dim)\n",
    "        # mean pooling, i.e. getting average representation of all tokens\n",
    "        pooled_output = seq_output.mean(axis=1)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        scores = self.classifier(pooled_output)  # (bs, num_classes)\n",
    "\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
