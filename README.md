# how-to-understand-bert ü§ó
In this repository, I have collected different sources, visualizations and code examples of BERT. I had been doing this during learning this material. Perhaps, I should have called this repository like "how-I-understand-bert" ü§î

## Theory üôå
So, I have started my long way in BERT with [blog posts](https://mccormickml.com/tutorials/) and [videos](https://www.youtube.com/watch?v=FKlPCK1uFrc&list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6&index=1) produced by Chris McCormick. After watching the first video, I clearly understood that it is necessary to read more about Attention and Transformer.

1. Attention
   + [en] [Visualizing A Neural Machine Translation Model](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) (Mechanics of Seq2seq Models With Attention) ‚ù§Ô∏è
   + [en] [Attention is all you need](https://arxiv.org/abs/1706.03762) - original paper
   + [en] [Attention is all you need](https://www.youtube.com/watch?v=iDulhoQ2pro) - short video about this paper
   + [en] [Attention is all you need](https://www.youtube.com/watch?v=nPuWGx_wF3I&list=WL&index=74) - shorter but better video ‚ù§Ô∏è
   + [ru] [Attention, attention!](https://www.youtube.com/watch?v=q9svwVYduSo&list=WL&index=98&t=4s) - seminar organized by JetBrains Research
2. Transformer
   + [en] [Awesome illustrations](http://jalammar.github.io/illustrated-transformer/) was created by Jay Alammar ‚ù§Ô∏è
   + [en] [The Narrated Transformer Language Model](https://www.youtube.com/watch?v=-QH8fRhqFHM) - explanation by Jay Alammar ‚ù§Ô∏è
   + [en] [Transformer (Attention is all you need)](https://www.youtube.com/watch?v=z1xs9jdZnuY) - short video with great visualizations
   + [en] [Great Jupyter Notebook](https://github.com/mertensu/transformer-tutorial) with Transformer visualization ‚ù§Ô∏è
   + [en] [Pytorch Transformers from Scratch](https://www.youtube.com/watch?v=U0s0f995w14&list=WL&index=40)
   + [en] Harvard NLP [full implementation in PyTorch](http://nlp.seas.harvard.edu/2018/04/03/attention.html) ‚ù§Ô∏è
   + [ru] [Self-Attention. Transformer overview](https://www.youtube.com/‚ù§Ô∏èwatch?v=UETKUIlYE6g) ‚ù§Ô∏è
   + [en] Just a short recap [video](https://www.youtube.com/watch?v=S27pHKBEp30&list=WL&index=95&t=41s)

## Examples üí™

## Sources üìö
* [en] [BERT classifier fine-tuning with PyTorch, HuggingFace, and Catalyst](https://github.com/Yorko/bert-finetuning-catalyst)

* [en] [Transformer encoder - visualized](https://github.com/mertensu/transformer-tutorial)

* [en] [Firing a cannon at sparrows: BERT vs. logreg](https://www.youtube.com/watch?v=JIU6WZuWl6k&list=WL&index=50)

* [en] The Illustrated BERT [blog post](http://jalammar.github.io/illustrated-bert/)

* [en] Tokenizers: [How machines read](https://blog.floydhub.com/tokenization-nlp/)

* [ru] [Self-Attention. Transformer overview](https://www.youtube.com/watch?v=UETKUIlYE6g)

* [ru] [Context based models. BERT overview](https://www.youtube.com/watch?v=1DygevyV2eA)

* [en] DistillBERT overview (distillation will be covered later in our course) [blog post](https://medium.com/huggingface/distilbert-8cf3380435b5)

* [en] Google AI Blog [post about open sourcing BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)

* [en] OpenAI blog post [Better Language Models and Their Implications (GPT-2)](https://openai.com/blog/better-language-models/)

* [en] One more [blog post explaining BERT](https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/)

* [en] Great PyTorch library: [pytorch-transformers](https://github.com/huggingface/transformers)

* [en] [Post about GPT-2 in OpenAI blog (by 04.10.2019)](https://openai.com/blog/fine-tuning-gpt-2/)

* [en] OpenAI API [request](https://openai.com/blog/openai-api/)
