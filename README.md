# how-to-understand-bert ðŸ¤—
In this repository, I have collected different knowledge of BERT and code examples

## Sources ðŸ“š
* [en] [BERT classifier fine-tuning with PyTorch, HuggingFace, and Catalyst](https://github.com/Yorko/bert-finetuning-catalyst)

* [en] [Transformer encoder - visualized](https://github.com/mertensu/transformer-tutorial)

* [en] [Firing a cannon at sparrows: BERT vs. logreg](https://www.youtube.com/watch?v=JIU6WZuWl6k&list=WL&index=50)

* [en] The Illustrated BERT [blog post](http://jalammar.github.io/illustrated-bert/)

* [en] DistillBERT overview (distillation will be covered later in our course) [blog post](https://medium.com/huggingface/distilbert-8cf3380435b5)

* [en] Google AI Blog [post about open sourcing BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)

* [en] OpenAI blog post [Better Language Models and Their Implications (GPT-2)](https://openai.com/blog/better-language-models/)

* [en] One more [blog post explaining BERT](https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/)

* [en] Great PyTorch library: [pytorch-transformers](https://github.com/huggingface/transformers)

* [en] [Post about GPT-2 in OpenAI blog (by 04.10.2019)](https://openai.com/blog/fine-tuning-gpt-2/)

* [en] OpenAI API [request](https://openai.com/blog/openai-api/)
